{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7e7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import os \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c87b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"output_language_text.txt\"\n",
    "MARKOV_ORDER = 2\n",
    "# some constants and file paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d37230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    def __init__(self, order=MARKOV_ORDER):\n",
    "        self.order = order\n",
    "        # transitions is a dictionary of prefixes i.e. tuple of length markov order, and value is the next word\n",
    "        self.transitions = {}\n",
    "\n",
    "    def train(self, sequence):\n",
    "        if len(sequence) < self.order + 1:\n",
    "            print(\"Sequence too short to train chain of this order\")\n",
    "            return\n",
    "        \n",
    "        for i in range(len(sequence) - self.order):\n",
    "            prefix = tuple(sequence[i:i+self.order])\n",
    "            next_item = sequence[i + self.order]\n",
    "\n",
    "            if(prefix not in self.transitions):\n",
    "                self.transitions[prefix] = []\n",
    "            self.transitions[prefix].append(next_item)\n",
    "\n",
    "    \n",
    "    def generate_next(self, current_prefix):\n",
    "        if current_prefix not in self.transitions:\n",
    "            all_possible_next_items = [item for sublist in self.transitions.values() for item in sublist]\n",
    "            if not all_possible_next_items:\n",
    "                raise StopIteration(\"Chain is stuck, no possible next state\")\n",
    "\n",
    "            return random.choice(all_possible_next_items)\n",
    "        \n",
    "        return random.choice(self.transitions[current_prefix])\n",
    "    \n",
    "def read_and_preprocess_data(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            text = text.replace(\"___\", \"\").strip()\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(\"Error reading file\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def tokenize_words(text):\n",
    "    tokens = re.findall(r'[\\w\\u0900-\\u097f]+', text, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    sentences = re.split(r'[.।?!]', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "def generate_text(chain, initial_sequence, length):\n",
    "    \"\"\"Generates a sequence of a given length using the trained chain.\"\"\"\n",
    "    \n",
    "    # Ensure the initial sequence is the correct size for the order\n",
    "    current_sequence = list(initial_sequence[-chain.order:])\n",
    "    generated_sequence = list(current_sequence)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        prefix = tuple(current_sequence)\n",
    "        \n",
    "        # If the model gets stuck and the fallback fails (very rare), break.\n",
    "        try:\n",
    "            # Generate the next item (handles unknown prefix internally)\n",
    "            next_item = chain.generate_next(prefix)\n",
    "        except StopIteration:\n",
    "            print(\"\\n(Chain stopped unexpectedly.)\", file=sys.stderr)\n",
    "            break\n",
    "\n",
    "        # Append to the generated text and update the sequence for the next prediction\n",
    "        generated_sequence.append(next_item)\n",
    "        current_sequence.pop(0)\n",
    "        current_sequence.append(next_item)\n",
    "        \n",
    "    # Return only the newly generated part (excluding the initial sequence)\n",
    "    return generated_sequence[chain.order:]\n",
    "\n",
    "\n",
    "\n",
    "def run_general_generation(data):\n",
    "    \"\"\"Implements Part A: Character, Word, and Sentence generation.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Part A: Character, Word, and Sentence Generation\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- 1. Character Generation ---\n",
    "    print(\"\\n--- 1. Character Generation (Length: 100) ---\")\n",
    "    char_chain = MarkovChain(order=2)\n",
    "    char_sequence = list(data)\n",
    "    char_chain.train(char_sequence)\n",
    "    \n",
    "    start_char_prefix = tuple(char_sequence[0:2])\n",
    "    generated_chars = generate_text(char_chain, start_char_prefix, 100)\n",
    "    print(\"\".join(generated_chars))\n",
    "\n",
    "    # --- 2. Word Generation ---\n",
    "    print(\"\\n--- 2. Word Generation (Length: 30 Words) ---\")\n",
    "    words = tokenize_words(data)\n",
    "    word_chain = MarkovChain(order=2)\n",
    "    word_chain.train(words)\n",
    "    \n",
    "    start_word_prefix = tuple(words[0:2])\n",
    "    generated_words = generate_text(word_chain, start_word_prefix, 30)\n",
    "    print(\" \".join(generated_words))\n",
    "\n",
    "    # --- 3. Sentence Generation ---\n",
    "    print(\"\\n--- 3. Sentence Generation (1 Sentence) ---\")\n",
    "    # Generate a single sentence by generating 10 words and hoping it sounds cohesive\n",
    "    words = tokenize_words(data)\n",
    "    word_chain = MarkovChain(order=3) # Higher order helps for better local coherence\n",
    "    word_chain.train(words)\n",
    "    \n",
    "    start_word_prefix = random.choice([tuple(words[i:i+3]) for i in range(len(words)-3)])\n",
    "    generated_words = generate_text(word_chain, start_word_prefix, 15)\n",
    "    \n",
    "    # Capitalize the first word and add a sentence ending\n",
    "    sentence_text = \" \".join(generated_words).capitalize() + \"।\"\n",
    "    print(sentence_text)\n",
    "\n",
    "\n",
    "\n",
    "def predict_next_words(word_chain, input_phrase, count=2):\n",
    "    \"\"\"Implements Part C: Predicts the next two words based on input.\"\"\"\n",
    "    \n",
    "    input_words = tokenize_words(input_phrase)\n",
    "    \n",
    "    if len(input_words) < word_chain.order:\n",
    "        print(f\"Error: Input phrase must be at least {word_chain.order} words long for this model order.\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Get the prefix (the last 'order' words)\n",
    "    start_prefix = tuple(input_words[-word_chain.order:])\n",
    "    \n",
    "    # Generate the words\n",
    "    next_words = generate_text(word_chain, start_prefix, count)\n",
    "    \n",
    "    return \" \".join(next_words)\n",
    "\n",
    "\n",
    "\n",
    "def predict_next_sentence(data, custom_query):\n",
    "    \"\"\"\n",
    "    Implements Part D: A simple conversational system based on sentence transitions.\n",
    "    It links one sentence to the next in the corpus and uses that as a Q&A pattern.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Part D: Conversational System (Next Sentence Prediction)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    sentences = tokenize_sentences(data)\n",
    "    if len(sentences) < 2:\n",
    "        print(\"Not enough sentences to build a conversational model.\")\n",
    "        return\n",
    "\n",
    "    # Train a Markov Chain where the state is the PREVIOUS sentence\n",
    "    # and the next item is the CURRENT sentence. This simulates conversation flow.\n",
    "    sentence_chain = MarkovChain(order=1)\n",
    "    \n",
    "    # Build a sequence of (Sentence1, Sentence2, Sentence3, ...)\n",
    "    # This chain will map: (Sentence_N) -> [Sentence_N+1]\n",
    "    sentence_chain.train(sentences)\n",
    "    \n",
    "    # Input/Output Examples\n",
    "    \n",
    "    # 1. Use a sentence directly from the corpus to find its 'answer'\n",
    "    query_1 = sentences[random.randint(0, len(sentences) - 2)]\n",
    "    \n",
    "    # The prefix is the full sentence itself\n",
    "    response_1 = sentence_chain.generate_next(tuple([query_1]))\n",
    "    \n",
    "    print(f\"Query (Corpus Sentence): \\\"{query_1}\\\"\")\n",
    "    print(f\"Response (Next Sentence): \\\"{response_1}\\\"\")\n",
    "    \n",
    "    # 2. Use a custom query and try to find the best match to initiate a response\n",
    "    custom_query = custom_query or \"जापान की राजधानी क्या है?\"  # What is the capital of Japan?\n",
    "    \n",
    "    # Find the sentence in the corpus that has the most matching words\n",
    "    best_match = max(sentences, key=lambda s: len(set(tokenize_words(custom_query)) & set(tokenize_words(s))))\n",
    "    \n",
    "    response_2 = sentence_chain.generate_next(tuple([best_match]))\n",
    "    \n",
    "    print(f\"\\nQuery (Custom Match): \\\"{custom_query}\\\"\")\n",
    "    print(f\"Best Corpus Match: \\\"{best_match}\\\"\")\n",
    "    print(f\"Response (Next Sentence): \\\"{response_2}\\\"\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3720fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Part A: Character, Word, and Sentence Generation\n",
      "==================================================\n",
      "\n",
      "--- 1. Character Generation (Length: 100) ---\n",
      " एकतीन और में जापान पर्म की से रूप रूप है।\n",
      "वर्व (Волов)\n",
      "2010 में) सक एक्षेत्तरीफ़िंग्रहवीपीय आपर्खा \n",
      "\n",
      "--- 2. Word Generation (Length: 30 Words) ---\n",
      "日本 निप्पोन या निहोन एशिया महाद्वीप के पूर्व में कनाडा की सीमा है। रूस की आधारशिला कहा जा सकता है। जापान की संस्कृति का अंधानुकरण किया है। बौद्ध धर्म यहां\n",
      "\n",
      "--- 3. Sentence Generation (1 Sentence) ---\n",
      "के रूप हैं जुड़वा अँगूठी motegi था होंडा द्वारा 1997 में पूरा करने के लिए।\n",
      "\n",
      "==================================================\n",
      "Part C: Next Two Words Prediction\n",
      "==================================================\n",
      "Input: \"संयुक्त राज्य अमेरिका\" --> Output: \"के संयुक्त\"\n",
      "Input: \"रूस एक\" --> Output: \"से के\"\n",
      "\n",
      "==================================================\n",
      "Part D: Conversational System (Next Sentence Prediction)\n",
      "==================================================\n",
      "Query (Corpus Sentence): \"जापान एक सबसे सफल एशिया में फुटबॉल टीमों में से एक है, एशियाई कप जीतने तीन बार\"\n",
      "Response (Next Sentence): \"गोल्फ भी जापान, के रूप में लोकप्रिय है सुपर जी\"\n",
      "\n",
      "Query (Custom Match): \"जापान की राजधानी क्या है?\"\n",
      "Best Corpus Match: \"जापान की राजधानी टोक्यो है और उसके अन्य बड़े महानगर योकोहामा, ओसाका, नागोया, साप्पोरो, फुकुओका, कोबे और क्योटो (जापान की पूर्ववर्ती राजधानी) हैं\"\n",
      "Response (Next Sentence): \"जापान दुनिया का ग्यारहवां सबसे अधिक जनसंख्या वाला देश है, साथ ही सबसे घनी जनसंख्या वाले और शहरीकृत देशों में से भी एक है\"\n"
     ]
    }
   ],
   "source": [
    "corpus_data = read_and_preprocess_data(DATA_FILE)\n",
    "if corpus_data: \n",
    "    run_general_generation(corpus_data)\n",
    "        \n",
    "        # --- Setup for Part C (Word Prediction) ---\n",
    "    words_for_c = tokenize_words(corpus_data)\n",
    "    word_chain_c = MarkovChain(order=2) # Order 2 is standard for word prediction\n",
    "    word_chain_c.train(words_for_c)\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Part C: Next Two Words Prediction\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "        # Example: Predict next two words for \"India is\" (Hindi equivalent)\n",
    "    input_phrase = \"संयुक्त राज्य अमेरिका\" # United States of America\n",
    "    next_two_words = predict_next_words(word_chain_c, input_phrase, count=2)\n",
    "        \n",
    "    print(f\"Input: \\\"{input_phrase}\\\" --> Output: \\\"{next_two_words}\\\"\")\n",
    "        \n",
    "        # Example 2\n",
    "    input_phrase_2 = \"रूस एक\" # Russia is a\n",
    "    next_two_words_2 = predict_next_words(word_chain_c, input_phrase_2, count=2)\n",
    "    print(f\"Input: \\\"{input_phrase_2}\\\" --> Output: \\\"{next_two_words_2}\\\"\")\n",
    "\n",
    "\n",
    "        # --- Run Part D (Conversational System) ---\n",
    "    predict_next_sentence(corpus_data)\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot run Markov generation without corpus data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae14036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete lol\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE2 = \"output_convo.txt\"\n",
    "from scraper import scrape_all\n",
    "urls_list_convo = [\n",
    "    \"https://www.reddit.com/r/selfhosted/comments/vw1oyi/a_static_and_simple_forum_solution/\",\n",
    "    \"https://www.reddit.com/r/devops/comments/1on5cjn/how_can_i_improve_my_kubernetes_and_cloud_skills/\",\n",
    "    \"https://www.reddit.com/r/Backend/comments/1ollqqr/suggest_a_good_backend_project_that_has_real_life/\",\n",
    "    \"https://www.reddit.com/r/Backend/comments/1olkzqn/confused_between_learning_java_spring_boot_or/\",\n",
    "    \"https://www.reddit.com/r/golang/comments/1nta34y/awesome_go_applications_open_source/\",\n",
    "    \"https://www.reddit.com/r/Compilers/comments/1oon6p5/handling_expressions_with_parsers/\",\n",
    "    \"https://www.reddit.com/r/Compilers/comments/1ojkq04/gpu_vs_ml_compiler_engineer/\",\n",
    "    \"https://www.reddit.com/r/Compilers/comments/1onc170/interview_for_a_ml_compiler_role_at_waymo/\",\n",
    "    \"https://www.reddit.com/r/Compilers/comments/1odbjq2/automated_generation_of_highlevel_code_from/\",\n",
    "    \"https://www.reddit.com/r/compsci/comments/1ooyxmj/seriously_llms_are_killing_captcha_need_2_mins_of/\",\n",
    "    \"https://www.reddit.com/r/compsci/comments/1oktglu/i_built_a_python_debugging_tool_that_uses/\",\n",
    "    \"https://www.reddit.com/r/compsci/comments/1oktglu/i_built_a_python_debugging_tool_that_uses/\",\n",
    "    \"https://www.reddit.com/r/ChatGPT/comments/1oovik0/i_trapped_an_llm_in_a_small_box_and_told_him_to/\",\n",
    "    \"https://www.reddit.com/r/developersIndia/comments/1ooq6br/indiabiz5_nontechnical_founder_struggling_with_my/\",\n",
    "    \"https://www.reddit.com/r/devops/comments/1ookpme/leetcode_style_interview_for_devops_role/\",\n",
    "    \"https://reddit.com/r/computerscience\",\n",
    "    \"https://reddit.com/r/compsci\",\n",
    "    \"https://reddit.com/r/cscareerquestions\",\n",
    "    \"https://reddit.com/r/cscareerquestionsOCE\",\n",
    "    \"https://reddit.com/r/cscareerquestionsEU\",\n",
    "    \"https://reddit.com/r/cscareerquestionsCAD\",\n",
    "    \"https://reddit.com/r/csMajors\",\n",
    "    \"https://reddit.com/r/cseducation\",\n",
    "    \"https://reddit.com/r/MSCS\",\n",
    "    \"https://reddit.com/r/cscareers\",\n",
    "    \"https://reddit.com/r/AskComputerScience\",\n",
    "    \"https://reddit.com/r/algorithms\",\n",
    "    \"https://reddit.com/r/programming\",\n",
    "    \"https://reddit.com/r/learnprogramming\",\n",
    "    \"https://reddit.com/r/coding\",\n",
    "    \"https://reddit.com/r/webdev\",\n",
    "    \"https://reddit.com/r/SideProject\",\n",
    "    \"https://reddit.com/r/javascript\",\n",
    "    \"https://reddit.com/r/learnjavascript\",\n",
    "    \"https://reddit.com/r/reactjs\",\n",
    "    \"https://reddit.com/r/database\",\n",
    "    \"https://reddit.com/r/mongodb\",\n",
    "    \"https://reddit.com/r/mysql\",\n",
    "    \"https://reddit.com/r/PostgreSQL\",\n",
    "    \"https://reddit.com/r/redis\",\n",
    "    \"https://reddit.com/r/datascience\",\n",
    "    \"https://reddit.com/r/datasets\",\n",
    "    \"https://reddit.com/r/machinelearning\",\n",
    "    \"https://reddit.com/r/MLQuestions\",\n",
    "    \"https://reddit.com/r/artificial\",\n",
    "    \"https://reddit.com/r/LanguageTechnology\",\n",
    "    \"https://reddit.com/r/computervision\",\n",
    "    \"https://reddit.com/r/networking\",\n",
    "    \"https://reddit.com/r/opensource\",\n",
    "    \"https://reddit.com/r/softwaredevelopment\",\n",
    "    \"https://reddit.com/r/tinycode\",\n",
    "    \"https://reddit.com/r/git\",\n",
    "    \"https://reddit.com/r/github\",\n",
    "    \"https://reddit.com/r/cpp\",\n",
    "    \"https://reddit.com/r/Cplusplus\",\n",
    "    \"https://reddit.com/r/LearnCpp\",\n",
    "    \"https://reddit.com/r/Cpp_questions\",\n",
    "    \"https://reddit.com/r/Csharp\",\n",
    "    \"https://reddit.com/r/dotnet\",\n",
    "    \"https://reddit.com/r/GameDev\",\n",
    "    \"https://reddit.com/r/truegamedev\",\n",
    "    \"https://reddit.com/r/UnrealEngine\",\n",
    "    \"https://reddit.com/r/Unity3D\",\n",
    "    \"https://reddit.com/r/godot\",\n",
    "    \"https://reddit.com/r/JustGameDevThings\",\n",
    "    \"https://reddit.com/r/bash\",\n",
    "    \"https://reddit.com/r/commandline\",\n",
    "    \"https://reddit.com/r/shell\",\n",
    "    \"https://reddit.com/r/emacs\",\n",
    "    \"https://reddit.com/r/neovim\",\n",
    "    \"https://reddit.com/r/vim\",\n",
    "    \"https://reddit.com/r/vscode\",\n",
    "    \"https://reddit.com/r/arduino\",\n",
    "    \"https://reddit.com/r/embedded\",\n",
    "    \"https://reddit.com/r/raspberry_pi\",\n",
    "    \"https://reddit.com/r/dotfiles\",\n",
    "    \"https://reddit.com/r/regex\",\n",
    "    \"https://reddit.com/r/FunctionalProgramming\",\n",
    "    \"https://reddit.com/r/ProgrammingLanguages\",\n",
    "    \"https://reddit.com/r/AskCompSci\",\n",
    "    \"https://reddit.com/r/DatabaseHelp\",\n",
    "    \"https://reddit.com/r/mariadb\",\n",
    "    \"https://reddit.com/r/RethinkDB\",\n",
    "    \"https://reddit.com/r/SQLServer\",\n",
    "    \"https://reddit.com/r/security\",\n",
    "    \"https://reddit.com/r/netsec\",\n",
    "    \"https://reddit.com/r/computerforensics\",\n",
    "    \"https://reddit.com/r/crypto\",\n",
    "    \"https://reddit.com/r/hackernews\",\n",
    "    \"https://reddit.com/r/dataisbeautiful\",\n",
    "    \"https://reddit.com/r/coolgithubprojects\",\n",
    "    \"https://reddit.com/r/softwaregore\",\n",
    "    \"https://reddit.com/r/unixporn\",\n",
    "    \"https://reddit.com/r/WatchPeopleCode\",\n",
    "    \"https://reddit.com/r/ProgrammerHumor\",\n",
    "    \"https://reddit.com/r/programmerreactions\",\n",
    "    \"https://reddit.com/r/itsaunixsystem\",\n",
    "    \"https://reddit.com/r/epochfail\",\n",
    "    \"https://reddit.com/r/RecruitingHell\",\n",
    "    \"https://reddit.com/r/WebDeveloperJobs\",\n",
    "    \"https://reddit.com/r/learnpython\",\n",
    "    \"https://reddit.com/r/python\",\n",
    "    \"https://reddit.com/r/java\",\n",
    "    \"https://reddit.com/r/csharp\",\n",
    "    \"https://reddit.com/r/swift\",\n",
    "    \"https://reddit.com/r/ruby\",\n",
    "    \"https://reddit.com/r/golang\",\n",
    "    \"https://reddit.com/r/scala\",\n",
    "    \"https://reddit.com/r/rust\",\n",
    "    \"https://reddit.com/r/c\",\n",
    "    \"https://reddit.com/r/objectivec\",\n",
    "    \"https://reddit.com/r/perl\",\n",
    "    \"https://reddit.com/r/php\",\n",
    "    \"https://reddit.com/r/typescript\",\n",
    "    \"https://reddit.com/r/assembly\",\n",
    "    \"https://reddit.com/r/dotnetcore\",\n",
    "    \"https://reddit.com/r/flutter\",\n",
    "    \"https://reddit.com/r/kotlin\"\n",
    "]\n",
    "\n",
    "scrape_all(urls_list_convo, DATA_FILE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b733c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Part A: Character, Word, and Sentence Generation\n",
      "==================================================\n",
      "\n",
      "--- 1. Character Generation (Length: 100) ---\n",
      "stilts-ons of) hation.\n",
      "        \n",
      "\n",
      "   Whobile off-prolowe sag a re entes.   \n",
      "\n",
      "\n",
      "          \n",
      "\n",
      "       \n",
      "\n",
      "  \n",
      "\n",
      "--- 2. Word Generation (Length: 30 Words) ---\n",
      "to advertise promote products or services or engage in such behavior will be removed based on ingredients available in the sub before posting If it did not create Git For\n",
      "\n",
      "--- 3. Sentence Generation (1 Sentence) ---\n",
      "Create your account and connect with a world of communities anyone can view post and।\n",
      "\n",
      "==================================================\n",
      "Part C: Next Two Words Prediction\n",
      "==================================================\n",
      "Input: \"help me self host on my old laptop\" --> Output: \"into board\"\n",
      "Input: \"kubernetes is a \" --> Output: \"TUI terminal\"\n",
      "\n",
      "==================================================\n",
      "Part D: Conversational System (Next Sentence Prediction)\n",
      "==================================================\n",
      "Query (Corpus Sentence): \"Say that AI is so incredibly effective and well developed in two years that it eliminates 50% of all work that we have to do\"\n",
      "Response (Next Sentence): \"Okay\"\n",
      "\n",
      "Query (Custom Match): \"this is a custom query\"\n",
      "Best Corpus Match: \"…and because this is a learning experience for me, I’m enlisting the assistance of ChatGPT to help me with this lol\n",
      "    \n",
      "\n",
      "      ChatGPT has become a versatile tool that many Redditors use in creative and practical ways daily\"\n",
      "Response (Next Sentence): \"Here are some of the most interesting and useful applications they've shared:\n",
      "    \n",
      "\n",
      "Storytelling and Creative Writing: Many users, including myself, use ChatGPT to help with creative writing, character development, and overcoming writer's block\"\n",
      "\n",
      "==================================================\n",
      "Part C: Next Two Words Prediction\n",
      "==================================================\n",
      "Input: \"help me self host on my old laptop\" --> Output: \"into board\"\n",
      "Input: \"kubernetes is a \" --> Output: \"TUI terminal\"\n",
      "\n",
      "==================================================\n",
      "Part D: Conversational System (Next Sentence Prediction)\n",
      "==================================================\n",
      "Query (Corpus Sentence): \"Say that AI is so incredibly effective and well developed in two years that it eliminates 50% of all work that we have to do\"\n",
      "Response (Next Sentence): \"Okay\"\n",
      "\n",
      "Query (Custom Match): \"this is a custom query\"\n",
      "Best Corpus Match: \"…and because this is a learning experience for me, I’m enlisting the assistance of ChatGPT to help me with this lol\n",
      "    \n",
      "\n",
      "      ChatGPT has become a versatile tool that many Redditors use in creative and practical ways daily\"\n",
      "Response (Next Sentence): \"Here are some of the most interesting and useful applications they've shared:\n",
      "    \n",
      "\n",
      "Storytelling and Creative Writing: Many users, including myself, use ChatGPT to help with creative writing, character development, and overcoming writer's block\"\n"
     ]
    }
   ],
   "source": [
    "corpus_data2 = read_and_preprocess_data(DATA_FILE2)\n",
    "if corpus_data2: \n",
    "    run_general_generation(corpus_data2)\n",
    "        \n",
    "        # --- Setup for Part C (Word Prediction) ---\n",
    "    words_for_c = tokenize_words(corpus_data2)\n",
    "    word_chain_c = MarkovChain(order=2) # Order 2 is standard for word prediction\n",
    "    word_chain_c.train(words_for_c)\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Part C: Next Two Words Prediction\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    input_phrase = \"help me self host on my old laptop\" # United States of America\n",
    "    next_two_words = predict_next_words(word_chain_c, input_phrase, count=2)\n",
    "        \n",
    "    print(f\"Input: \\\"{input_phrase}\\\" --> Output: \\\"{next_two_words}\\\"\")\n",
    "        \n",
    "        # Example 2\n",
    "    input_phrase_2 = \"kubernetes is a \"\n",
    "    next_two_words_2 = predict_next_words(word_chain_c, input_phrase_2, count=2)\n",
    "    print(f\"Input: \\\"{input_phrase_2}\\\" --> Output: \\\"{next_two_words_2}\\\"\")\n",
    "\n",
    "\n",
    "        # --- Run Part D (Conversational System) ---\n",
    "    predict_next_sentence(corpus_data2, \"this is a custom query\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot run Markov generation without corpus data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "markov_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
